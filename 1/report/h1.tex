\documentclass[12pt]{article}
\usepackage[paper=letterpaper,margin=2cm]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{newtxtext, newtxmath}
\usepackage{enumitem}
\usepackage{titling}
\usepackage[colorlinks=true]{hyperref}
\usepackage{multirow}
\usepackage{svg}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{float}
\usepackage{graphicx}
\usepackage{ragged2e}
\usepackage{subcaption}

\setlength{\droptitle}{-6em}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    commentstyle=\color{codegreen},
    keepspaces=true,
    keywordstyle=\color{magenta},
    numbers=left,
    numbersep=5pt,
    numberstyle=\tiny\color{codegray},
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    stringstyle=\color{codepurple},
    tabsize=4
}

\lstset{style=mystyle}

\title{\large{Deep Learning 2023}\vskip 0.2cm Homework 1 -- Group 48}
\date{}
\author{Miguel Vale (99113) \and Pedro Lobo (99115)}
\begin{document}
\maketitle

\center\large{\vskip -1.0cm\textbf{Contribution}}

\begin{justify}
    We started by dividing the coding part of this homework by the two of us. Questions 1.1. a), 1.1. b) and 2.2 were done by Miguel while questions 1.2. b) and 2.1 were done by Pedro. Although we distributed this questions between the both of us, we double-checked each other's implementation.

    Questions 1.2 a) was done by Pedro and later reviewed by Miguel while question 3.1 a) was done by Miguel and later reviewed by Pedro. The remaining questions, namely questions 3.1 b) and 3.1 c), were done by the two of us.

    The report was written by the two of us, with each of us writing the answers to the questions that we implemented and then reviewed by the other.
\end{justify}

\center\large{\vskip 0.5cm\textbf{Question 1}}
\center\textbf{Medical image classification with linear classifiers and neural networks}
\begin{enumerate}[leftmargin=\labelsep]

    \item
          \begin{enumerate}[label=\alph*)]

              \item \textit{Implement the \texttt{update\_weights} method of the \texttt{Perceptron} class in \texttt{hw1-q1.py}. Then train 20 epochs of the perceptron on the training set and report its performance on the training, validation and test sets. Plot the train and validation accuracies as a function of the epoch number.}

                    \vspace{12pt}

                    The resulting test accuracy for the perceptron is 0.3422. The train and validation accuracies are depicted in Figure \ref{fig:perceptron-20-1.0}.

                    \begin{figure}[H]
                        \centering
                        \includesvg[width=0.75\textwidth]{./1/report/assets/q1-perceptron-20-1.0.svg}
                        \caption{Perceptron train and test accuracies as a function of the epoch number.}
                        \label{fig:perceptron-20-1.0}
                    \end{figure}

                    \vspace{12pt}

              \item \textit{Repeat the same exercise using logistic regression instead (without regularization), using stochastic gradient descent as your training algorithm.  Report the final test accuracies and compare, based on the plots of the train and validation accuracies, the models obtained using two different learning rates of $\eta$ = 0.01 and $\eta$ = 0.001.}

                    \vspace{12pt}

                    For a learning rate of $\eta = 0.01$, the resulting test accuracy is 0.5784, while for a learning rate of $\eta = 0.001$, the obtained test accuracy is 0.5936. The train and validation accuracies for $\eta = 0.01$ and $\eta = 0.001$ are depicted in Figures \ref{fig:logistic_regression-50-0.01} and \ref{fig:logistic_regression-50-0.001}, respectively.

                    \begin{figure}[H]
                        \centering
                        \includesvg[width=0.75\textwidth]{./1/report/assets/q1-logistic_regression-50-0.01.svg}
                        \caption{Logistic regression train and validation accuracies as a function of the epoch number for $\eta = 0.01$.}
                        \label{fig:logistic_regression-50-0.01}
                    \end{figure}

                    \begin{figure}[H]
                        \centering
                        \includesvg[width=0.75\textwidth]{./1/report/assets/q1-logistic_regression-50-0.001.svg}
                        \caption{Logistic regression training and validation accuracies as a function of the epoch number for $\eta = 0.001$.}
                        \label{fig:logistic_regression-50-0.001}
                    \end{figure}

          \end{enumerate}

    \item

          \begin{enumerate}[label=\alph*)]

              \item \textit{Comment the following claim: “A logistic regression model using pixel values
                        as features is not as expressive as a multi-layer perceptron using \texttt{ReLU} activations.
                        However, training a logistic regression model is easier because it is a convex optimization
                        problem.” Is this true of false? Justify.}

                    \vspace{12pt}

                    The claim is correct. The following two paragraphs address the expressiveness of the models and their optimization problems, respectively.

                    \vspace{12pt}

                    A multi-layer perceptron using ReLU activations is more expressive than a logistic regression model as its hidden layers have a non-linear activation which allows the model to learn more complex representations of the input. This more complex representations can turn the input into a linearly separable dataset, that can be learned by the output layer. A logistic regression model can only learn linearly separable datasets, as its output layer is a linear function of the input.

                    \vspace{12pt}

                    Despite being more expressive, a multi-layer perceptron using ReLU activations is harder to train than a logistic regression model. This is due to the fact that the optimization problem of the former is non-convex, while the optimization problem of the latter is convex. This means that the optimization problem of a logistic regression model has a single global minimum, while the optimization problem of a multi-layer perceptron using ReLU activations, in general, can have multiple local minima. While training the multi-layer perceptron with a gradient descent algorithm, the model can get \textit{stuck} on a local minimum. This makes it harder to find the optimal solution for the latter model.

                    \vspace{12pt}

              \item \textit{Without using any neural network toolkit, implement a multi-layer
                    perceptron with a single hidden layer to solve this problem, including the gradient
                    backpropagation algorithm which is needed to train the model. Use 200 hidden units,
                    a ReLU activation function for the hidden layers, and a multinomial logistic loss (also
                    called cross-entropy) in the output layer (even though we are dealing with a binary
                    classification this will allow you to use the same code for a multi-class problem). Don’t
                    forget to include bias terms in your hidden units. Train the model for 20 epochs with
                    stochastic gradient descent with a learning rate of 0.001. Initialize biases with zero
                    vectors and values in weight matrices with $w_{ij} \sim \mathcal{N}(\mu, \sigma^2)$ with $\mu = 0.1$ and $\sigma^2 = {0.1}^2$. Report final test accuracy and include the plots of the train loss and train and validation
                    accuracies as a function of the epoch number.
                    }

                    \vspace{12pt}

                    For a learning rate of $\eta = 0.001$, the resulting test accuracy for the MLP is 0.6106. The correspondant train and validation accuracies are depicted in Figure \ref{fig:mlp-20-0.001}.

                    \begin{figure}[H]
                        \centering
                        \includesvg[width=0.75\textwidth]{./1/report/assets/q1-mlp-20-0.001.svg}
                        \caption{MLP train and validation accuracies as a function of the epoch number for $\eta = 0.001$.}
                        \label{fig:mlp-20-0.001}
                    \end{figure}

                    \vspace{12pt}

          \end{enumerate}

\end{enumerate}

\vspace{12pt}

\center\large{\textbf{Question 2}}
\center\textbf{Medical image classification with an autodiff toolkit}

\begin{enumerate}[leftmargin=\labelsep]
    \item \textit{Implement a linear model with logistic regression, using stochastic gradient descent as your training algorithm (use a batch size of 16). Train your model for 20 epochs and tune the learning rate on your validation data, using the following values: $\{0.001, 0.01, 0.1\}$.  Report the best configuration (in terms of final validation accuracy) and plot two things: the training loss and the validation accuracy, both as a function of the epoch number. Report the final accuracy on the test set.}

          \vspace{12pt}

          The best configuration, in term of final validation accuracy, is $\eta = 0.001$, as shown in Table \ref{tab:logistic_regression-20}.

          \begin{table}[H]
              \centering
              \begin{tabular}{|c|c|}
                  \hline
                  Learning rate & Validation accuracy \\ \hline
                  0.001         & 0.6503              \\ \hline
                  0.01          & 0.6200              \\ \hline
                  0.1           & 0.5577              \\ \hline
              \end{tabular}
              \caption{Final logistic regression validation accuracy for each learning rate.}
              \label{tab:logistic_regression-20}
          \end{table}

          The traning loss and validation accuracy plots for a learning rate of $\eta = 0.001$ are depicted in Figures \ref{fig:q2-logistic_regression-training-loss-batch-16-lr-0.001-epochs-20-l2-0.0-opt-sgd} and \ref{fig:q2-logistic_regression-validation-accuracy-batch-16-lr-0.001-epochs-20-l2-0.0-opt-sgd}, respectively.

          \begin{figure}[H]
              \centering
              \includesvg[width=0.7\textwidth]{./1/report/assets/q2-logistic_regression-training-loss-batch-16-lr-0.001-epochs-20-l2-0.0-opt-sgd}
              \caption{Logistic regression training loss for a learning rate of $\eta = 0.001$.}
              \label{fig:q2-logistic_regression-training-loss-batch-16-lr-0.001-epochs-20-l2-0.0-opt-sgd}
          \end{figure}

          \begin{figure}[H]
              \centering
              \includesvg[width=0.7\textwidth]{./1/report/assets/q2-logistic_regression-validation-accuracy-batch-16-lr-0.001-epochs-20-l2-0.0-opt-sgd}
              \caption{Logistic regression validation accuracy for a learning rate of $\eta = 0.001$.}
              \label{fig:q2-logistic_regression-validation-accuracy-batch-16-lr-0.001-epochs-20-l2-0.0-opt-sgd}
          \end{figure}

          \vspace{12pt}

    \item \textit{Implement a feed-forward neural network using dropout regularization. Make
              sure to include all the hyperparameters and training/model design choices shown in Table
                  {\ref{tab:mlp-default}}. Use the values presented in the table as default.}

          \begin{table}[H]
              \centering
              \begin{tabular}{|c|c|}
                  \hline
                  \textbf{Hyperparameter} & \textbf{Value} \\ \hline
                  Number of Epochs        & 20             \\ \hline
                  Learning Rate           & 0.1            \\ \hline
                  Hidden Size             & 200            \\ \hline
                  Number of Layers        & 2              \\ \hline
                  Dropout                 & 0.0            \\ \hline
                  Batch Size              & 16             \\ \hline
                  Activation              & ReLU           \\ \hline
                  L2 Regularization       & 0.0            \\ \hline
                  Optimizer               & SGD            \\ \hline
              \end{tabular}
              \caption{Default hyperparameters and training/model design choices for the MLP.}
              \label{tab:mlp-default}
          \end{table}

          \begin{enumerate}[label=\alph*)]
              \item \textit{Compare the performance of your model with batch sizes 16 and 1024 with
                        the remaining hyperparameters at their default value. Plot the train and validation
                        losses for both, report the best test accuracy and comment on the differences in both
                        performance and time of execution.}

                    \vspace{12pt}

                    The configuration with a batch size of 16 has the best final test accuracy, as shown in Table \ref{tab:mlp-batch-size-accuracy}.

                    \vspace{12pt}

                    \begin{table}[H]
                        \centering
                        \begin{tabular}{|c|c|}
                            \hline
                            Batch size & Test accuracy \\ \hline
                            16         & 0.7713        \\ \hline
                            1024       & 0.7202        \\ \hline
                        \end{tabular}
                        \caption{Final MLP test accuracy for each batch size.}
                        \label{tab:mlp-batch-size-accuracy}
                    \end{table}

                    The configuration with a batch size of 1024 takes less time to train, as shown in Table \ref{tab:mlp-batch-size-time}.

                    \vspace{12pt}

                    \begin{table}[H]
                        \centering
                        \begin{tabular}{|c|c|}
                            \hline
                            Batch size & Time (seconds) \\ \hline
                            16         & 242            \\ \hline
                            1024       & 67             \\ \hline
                        \end{tabular}
                        \caption{Training time of the MLP for each batch size.}
                        \label{tab:mlp-batch-size-time}
                    \end{table}

                    The train and validation losses for a batch size of $16$ and a batch size of $1024$ are depicted in Figures \ref{fig:q2-mlp-training-loss-batch-16-lr-0.1-epochs-20-hidden-200-dropout-0.0-l2-0.0-layers-2-act-relu-opt-sgd} and \ref{fig:q2-mlp-training-loss-batch-1024-lr-0.1-epochs-20-hidden-200-dropout-0.0-l2-0.0-layers-2-act-relu-opt-sgd}, respectively.

                    \begin{figure}[H]
                        \centering
                        \includesvg[width=0.75\textwidth]{./1/report/assets/q2-mlp-training-loss-batch-16-lr-0.1-epochs-20-hidden-200-dropout-0.0-l2-0.0-layers-2-act-relu-opt-sgd}
                        \caption{MLP train and validation losses for a batch size of 16.}
                        \label{fig:q2-mlp-training-loss-batch-16-lr-0.1-epochs-20-hidden-200-dropout-0.0-l2-0.0-layers-2-act-relu-opt-sgd}
                    \end{figure}

                    \begin{figure}[H]
                        \centering
                        \includesvg[width=0.75\textwidth]{./1/report/assets/q2-mlp-training-loss-batch-1024-lr-0.1-epochs-20-hidden-200-dropout-0.0-l2-0.0-layers-2-act-relu-opt-sgd}
                        \caption{MLP train and validation losses for a batch size of 1024.}
                        \label{fig:q2-mlp-training-loss-batch-1024-lr-0.1-epochs-20-hidden-200-dropout-0.0-l2-0.0-layers-2-act-relu-opt-sgd}
                    \end{figure}

                    The model with a batch size of 16 takes longer to train. This is due to the fact that the gradient computed for each batch is more \textit{noisy} (it is less aligned with the true gradient) than the gradient computed for a batch size of 1024. This means that the stochastic gradient descent algorithm takes longer to converge as it takes more steps to reach the optimal solution. A larger batch size leads to a more accurate estimate of the gradient, which leads to a faster convergence of the stochastic gradient descent algorithm and a faster training time.

                    The plot of the train and validation losses for a batch size of 1024 shows a smoother training process. This is also indicative of a more accurate gradient computation for each batch.

                    \vspace{12pt}

                    The model with a batch size of 16 has a higher test accuracy, relative to the model with a batch size of 1024. This is due to the fact that a small batch size can restrict the training of the model, as it introduces more variety into the training process. This can lead to a better generalization of the model, as it is more robust to unseen data, avoiding overfitting. Models less prone to overfitting show better performance on the test set.

                    \vspace{12pt}

              \item \textit{Train the model with learning rates: 1, 0.1, 0.01 and 0.001 with the remaining
                        hyperparameters at their default value. Plot the train and validation losses for the best
                        and worst configurations in terms of validation accuracy, report the best test accuracy
                        and comment on the differences in performance.}

                    \vspace{12pt}

                    The best and worst configuration, with regard to the test accuracy, are $\eta = 1$ and $\eta = 0.1$, respectively, as shown in Table \ref{tab:mlp-learning-rate-accuracy}.

                    \begin{table}[H]
                        \centering
                        \begin{tabular}{|c|c|}
                            \hline
                            Learning rate & Test accuracy \\ \hline
                            1             & 0.4726        \\ \hline
                            0.1           & 0.7713        \\ \hline
                            0.01          & 0.7543        \\ \hline
                            0.001         & 0.6900        \\ \hline
                        \end{tabular}
                        \caption{Test accuracy of the MLP for each learning rate.}
                        \label{tab:mlp-learning-rate-accuracy}
                    \end{table}

                    The train and validations losses for the configurations with $\eta = 1$ and $\eta = 0.1$ are depicted in Figures \ref{fig:q2-mlp-training-loss-batch-16-lr-1.0-epochs-20-l2-0.0-opt-sgd} and \ref{fig:q2-mlp-training-loss-batch-16-lr-0.1-epochs-20-l2-0.0-opt-sgd}, respectively.

                    \begin{figure}[H]
                        \centering
                        \includesvg[width=0.75\textwidth]{./1/report/assets/q2-mlp-training-loss-batch-16-lr-1.0-epochs-20-hidden-200-dropout-0.0-l2-0.0-layers-2-act-relu-opt-sgd}
                        \caption{MLP train and validation losses for a learning rate of $\eta = 1.0$.}
                        \label{fig:q2-mlp-training-loss-batch-16-lr-1.0-epochs-20-l2-0.0-opt-sgd}
                    \end{figure}

                    \begin{figure}[H]
                        \centering
                        \includesvg[width=0.75\textwidth]{./1/report/assets/q2-mlp-training-loss-batch-16-lr-0.1-epochs-20-hidden-200-dropout-0.0-l2-0.0-layers-2-act-relu-opt-sgd}
                        \caption{MLP train and validation losses for a learning rate of $\eta = 0.1$.}
                        \label{fig:q2-mlp-training-loss-batch-16-lr-0.1-epochs-20-l2-0.0-opt-sgd}
                    \end{figure}

                    The models with a very large or very small learning rate exhibit worst test accuracy.

                    \vspace{12pt}

                    A small learning rate can introduce problems into the training process. Some problems include a slow convergence of the stochastic gradient descent algorithm and a higher chance of getting stuck in a local minimum. As the learning rate is small, the model takes \textit{little steps} taking a lot of training epochs to reach the optimal solution.

                    \vspace{12pt}

                    On the other hand, a large learning rate can lead to an overshooting of the optimal solution, as the model takes \textit{big steps} in the direction of the gradient. This can lead to the model \textit{bouncing} around the optimal solution, but never reaching it.

                    \vspace{12pt}

                    Models that have a learning rate that is nor too small nor too large mitigate some of this problems, displaying a better test accuracy.

                    \vspace{12pt}

              \item \textit{Using a batch size of 256 run the default model for 150 epochs. Is there
                        overfitting?
                        Train two similar models with the following changes: one with the L2
                        regularization parameter set to 0.0001 and the other with a dropout probability of 0.2.
                        Plot the train and validation losses for the best and worst configuration in terms of
                        validation accuracy, report the best test accuracy and comment on the differences of
                        both techniques.}

                    \vspace{12pt}

                    The best and worst configuration with regard to the validation accuracy are the default model (with a batch size of 256) and the model with a dropout probability $= 0.001$, respectively, as shown in Table \ref{tab:mlp-configuration-accuracy}.

                    \begin{table}[H]
                        \centering
                        \begin{tabular}{|c|c|}
                            \hline
                            Model                                & Test accuracy \\ \hline
                            Default with batch size = 256        & 0.7713        \\ \hline
                            L2 regularization parameter = 0.0001 & 0.7788        \\ \hline
                            Dropout probability = 0.2            & 0.7996        \\ \hline
                        \end{tabular}
                        \caption{Final MLP test accuracy for one of the models.}
                        \label{tab:mlp-configuration-accuracy}
                    \end{table}


                    The train and validation losses for the configuration with a batch size of 256 and the configuration with a dropout probability of 0.2 are depicted in Figures \ref{fig:q2-mlp-training-loss-batch-256-lr-0.01-epochs-150-hidden-200-dropout-0.0-l2-0.0-layers-2-act-relu-opt-sgd} and \ref{fig:q2-mlp-training-loss-batch-256-lr-0.01-epochs-150-hidden-200-dropout-0.2-l2-0.0-layers-2-act-relu-opt-sgd}, respectively.

                    \begin{figure}[H]
                        \centering
                        \includesvg[width=0.75\textwidth]{./1/report/assets/q2-mlp-training-loss-batch-256-lr-0.1-epochs-150-hidden-200-dropout-0.0-l2-0.0-layers-2-act-relu-opt-sgd}
                        \caption{MLP train and validation losses for the default model, with a batch size of 256.}
                        \label{fig:q2-mlp-training-loss-batch-256-lr-0.01-epochs-150-hidden-200-dropout-0.0-l2-0.0-layers-2-act-relu-opt-sgd}
                    \end{figure}

                    \begin{figure}[H]
                        \centering
                        \includesvg[width=0.75\textwidth]{./1/report/assets/q2-mlp-training-loss-batch-256-lr-0.1-epochs-150-hidden-200-dropout-0.2-l2-0.0-layers-2-act-relu-opt-sgd}
                        \caption{MLP train and validation losses for a dropout probability of 0.2.}
                        \label{fig:q2-mlp-training-loss-batch-256-lr-0.01-epochs-150-hidden-200-dropout-0.2-l2-0.0-layers-2-act-relu-opt-sgd}
                    \end{figure}

                    The configurations differ as the default model doesn't have any regularization technique, while the model with a dropout probability of 0.2 uses dropout regularization. The random drop of certain connections forces the network to learn a more robust representation of the data, as it cannot rely on the presence of a particular unit.

                    \vspace{12pt}

                    The presence of a regularization technique leads to lower variations of the validation accuracy and validation loss between epochs. It also translates into a higher test loss at the cost of a lower validation loss. As the model is more restricted in the training process, it is more capable of generalizing for unseen data. The presence of dropout regularization reduces overfitting.

                    \vspace{12pt}

          \end{enumerate}
\end{enumerate}

\center\large{\textbf{Question 3}}

\begin{enumerate}[leftmargin=\labelsep]
    \item \textit{In this exercise, you will design a multilayer perceptron to compute a Boolean function of $D$ variables, $f; \{-1, +1\}^D \mapsto \{-1, +1\}$, defined as:}
          \begin{equation*}
              f(x) = \begin{cases}
                  +1 & \text{if } \sum\limits_{i=1}^D x_i \in [A, B], \\
                  -1 & \text{otherwise}
              \end{cases}
          \end{equation*}
          \textit{where $A$ and $B$ are integers such that $-D \leq A \leq B \leq D$.}

          \begin{enumerate}[label=\alph*)]
              \item \textit{Show that the function above cannot generally be computed with a single
                        perceptron.}

                    \vspace{12pt}

                    If we consider $D$ = 2, $A$ = -1 and $B$ = 1, the function $f(x)$ is mapped into the XOR function, as suggested by the following truth table:

                    \begin{table}[H]
                        \centering
                        \begin{tabular}{|c|c|c|c|}
                            \hline
                            $x_1$ & $x_2$ & $\sum\limits_{i=1}^D x_i$ & $f(x)$ \\ \hline
                            -1    & -1    & -2                        & -1     \\ \hline
                            -1    & 1     & 0                         & 1      \\ \hline
                            1     & -1    & 0                         & 1      \\ \hline
                            1     & 1     & 2                         & -1     \\ \hline
                        \end{tabular}
                    \end{table}

                    A single perceptron can only learn linearly separable functions. As the XOR function is not linearly separable, it cannot be learned by a single perceptron, as there is no hyperplane that can separate the two classes, as suggested by Figure \ref{fig:xor}.

                    \begin{figure}[H]
                        \centering
                        \includesvg[width=0.75\textwidth]{./1/report/assets/xor.svg}
                        \caption{Plot of function $f$ with $D$ = 2, $A$ = -1 and $B$ = 1. Red points represent the class -1, while blue points represent the class 1. The two classes are not linearly separable.}
                        \label{fig:xor}
                    \end{figure}

                    \vspace{12pt}


              \item \textit{Show that the function above can be computed with a multilayer perceptron with a single hidden layer with two hidden units and hard threshold activations $g: \mathcal{R} \rightarrow \{-1, +1\}$ with}
                    \begin{equation*}
                        g(z) = \text{sign}(z) = \begin{cases}
                            +1 & \text{if } z \geq 0 \\
                            -1 & \text{otherwise}
                        \end{cases}
                    \end{equation*}
                    \textit{and where \textbf{all the weights and biases are integers} (positive, negative, or zero).Provide all the weights and biases of such network, and ensure that the resulting network is robust to infinitesimal perturbation of the inputs, i.e., the resulting function $h: \mathcal{R}^D \rightarrow \mathcal{R}$ should be such that $\lim\limits_{t \rightarrow 0} h(x + tv) = h(x) = f(x)$ for any $x \in \{-1, +1\}$ and $v \in \mathcal{R}^D$.}

                    \vspace{12pt}



              \item \textit{}
          \end{enumerate}
\end{enumerate}

\end{document}
