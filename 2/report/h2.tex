\documentclass[12pt]{article}
\usepackage[paper=letterpaper,margin=2cm]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{newtxtext, newtxmath}
\usepackage{enumitem}
\usepackage{titling}
\usepackage[colorlinks=true]{hyperref}
\usepackage{multirow}
\usepackage{svg}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{float}
\usepackage{graphicx}
\usepackage{ragged2e}
\usepackage{subcaption}

\setlength{\droptitle}{-6em}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    commentstyle=\color{codegreen},
    keepspaces=true,
    keywordstyle=\color{magenta},
    numbers=left,
    numbersep=5pt,
    numberstyle=\tiny\color{codegray},
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    stringstyle=\color{codepurple},
    tabsize=4
}

\lstset{style=mystyle}

\title{\large{Deep Learning 2023}\vskip 0.2cm Homework 2 -- Group 48}
\date{}
\author{Miguel Vale (99113) \and Pedro Lobo (99115)}
\begin{document}
\maketitle

\center\large{\vskip -1.0cm\textbf{Contribution}}

\begin{justify}
    We started by dividing the coding part of this homework by the two of us. Questions 2 was done by Pedro while question 3 was done by Miguel. Although we distributed this questions between the both of us, we double-checked each other's implementation.

    Question 1 was tackled by the two of us.

    The report was written by the two of us, with each of us writing the answers to the questions that we implemented and then reviewed by the other.
\end{justify}

\center\large{\vskip 0.5cm\textbf{Question 1}}
\begin{enumerate}[leftmargin=\labelsep]

    \item \textit{Consider the self-attention layer of a transformer with a single attention head, which performs the computation $\mathbf{Z}$ = $\text{Softmax}(\mathbf{QK^T})\mathbf{V}$, where $\mathbf{Q} \in \mathbb{R}^{L \times D}$, $\mathbf{K} \in \mathbb{R}^{L \times D}$, $\mathbf{V} \in \mathbb{R}^{L \times D}$, for a long sequence length $L$ and hidden size $D$. What is the computation complexity, in terms of L, of computing $\mathbf{Z}$? Briefly explain why this could be problematic for long sequences.}

          \vspace{12pt}

          Each line of the matrix $\mathbf{Q}$ is multiplied by each line of the matrix $\mathbf{K}$, with each multiplication being a operation of size $D$. So, the total cost of the matrix multiplication is $\mathcal{O} (L \times L \times D)$.

          \vspace{12pt}

          Then, the softmax function is applied to each element of the resulting matrix, which has size $L \times L$.

          \vspace{12pt}

          Finally, the matrix $\mathbf{V} \in \mathbb{R}^{L \times D}$ is multiplied by the resulting matrix $\in \mathbb{R}^{L \times D}$, which as a cost of $\mathcal{O}(L^2D)$.

          \vspace{12pt}

          So, the total cost of computing $\mathbb{Z}$ is $\mathcal{O} (L^2 D)$.

          \vspace{12pt}

          This is problematic for long sequences because the cost of computing $\mathbb{Z}$ grows quadratically with the length of the sequence, which means that the time and space required to compute $\mathbb{Z}$ for longer sequences grows very quickly.

    \item \textit{We will show that by suitably approximating the softmax transformation the computational complexity above can be reduced. Consider the McLaurin series expansion for the exponential function,}
          \begin{equation*}
              exp(t) = \sum_{n=0}^{\infty} \frac{t^n}{n!} = 1 + t + \frac{t^2}{2!} + \frac{t^3}{6} + \dots
          \end{equation*}
          \textit{Using only the first three terms in this series, we obtain $\text{exp}(t) \approx 1 + t + \frac{t^2}{2}$. Use this approximation to obtain a feature map $\phi: \mathbb{R}^D \rightarrow \mathbb{R}^M$ such that, for arbitrary $\textbf{q} \in \mathbb{R}^D$ and, $\textbf{k} \in \mathbb{R}^D$, we have $\text{exp}(\textbf{q}^T\textbf{k}) \approx \phi(\textbf{q})^T\phi(\textbf{k})$. Express the dimensionality of the feature space $M$ as a function of $D$. What would be the dimensionality if you used $K \ge 3$ terms in the McLaurin series expansion (as a function of $D$ and $K$)?}

          \vspace{12pt}

          $M = 1 + D + \frac{D \times (D - 1)}{2}$, where 1 is a constant term, $D$ is linear to the number of features and $\frac{D \times (D - 1)}{2}$ is the number of possible combinations of two features, only excluding the combinations of a feature with itself.

          \vspace{12pt}

          If $K \ge 3$, $t^{k-1}$ will include all the features up to $k-1$, so
          \begin{equation}
              M = \sum_{i}^{k-1} {D + k - 1 \choose k}
          \end{equation}
          Therefore,
          \begin{equation}
              M = \sum_{i}^{k-1} \frac{(D + i - 1)!}{i!(D - 1)!} = 1 + \frac{D!}{(D-1)!}
          \end{equation}

    \item \textit{Using the approximation $\text{exp}(\textbf{q}^Tk) \approx \phi(\textbf{q})^T \phi(\textbf{k})$, and denoting by $\Phi({\textbf{Q}}) \in \mathbb{R}^{L \times M}$ and $\Phi({\textbf{K}}) \in \mathbb{R}^{L \times M}$ the matrices whose rows are (respectively) $\phi(\textbf{k}_i)$, where $\textbf{q}_i$ and $\textbf{k}_i$ denote (also respectively) the $\text{i}^{\text{th}}$ rows of the original matrices $\textbf{Q}$ and $\textbf{K}$, show that the self-attention operation can be approximated as $\mathbf{Z} \approx \mathbf{D}^{-1}\Phi({\textbf{Q}})\Phi({\textbf{K}})^T\mathbf{V}$, where $\mathbf{D} = \textbf{Diag}(\Phi({\textbf{Q}})\Phi({\textbf{K}})^T\mathbf{1}_L)$ (here, $\textbf{Diag}(v)$ denotes a matrix with the entries of vector $v$ in the diagonal, and $\mathbf{1}_L$ denotes a vector of of ones with size of length $L$).}

          \vspace{12pt}

          As vector $v$ is the product between $\Phi(\textbf{Q})$ and $\Phi(\textbf{K})^T$, it is a vector where each entry is the sum of the correspondent line in $\Phi(\textbf{Q})\Phi(\textbf{K})$.

          \vspace{12pt}

          The diagonal matrix $\textbf{Diag}(v) \in \mathbb{R}^{L \times L}$ is a matrix with the vector $v$ in the diagonal. As the matrix is diagonal, its inverse can be calculated by taking the inverse of each of the elements in the diagonal. So, $D^{-1} \Phi(\textbf{Q}) \Phi(\textbf{K})^T$ will work as a softmax function, as the inverse of the sum of the elements in each line of the matrix will be multiplied by the correspondent line of $\Phi(\textbf{Q}) \Phi(\textbf{K})^T$.

          \vspace{12pt}

          \begin{equation}
              \text{Softmax}(\textbf{QK}^T)\textbf{V} = \frac{e^{\textbf{QK}^T}}{\sum e^{\textbf{QK}^T}} \cdot \textbf{V} \approx D^{-1} \Phi(\textbf{Q}) \Phi(\textbf{K})^T \textbf{V}
          \end{equation}

    \item \textit{Show how we can exploit the above approximation to obtain a computational complexity which is linear in $L$. How does the computational complexity depend on $M$ and $D$?}

          \vspace{12pt}

          $\textbf{Q} \in \mathbb{R}^{L \times D}$ and $\phi: \mathbb{R}^D \rightarrow \mathbb{R}^M$, so $\phi(\textbf{q}_i) \mapsto \Phi(\textbf{Q}) \in \mathbb{R}^{L \times M}$, which means that the computational complexity is in the order of $\mathcal{O}(L \times D \times M)$, which is linear in $L$.

          \vspace{12pt}

          As $M << D$, the computational cost of $\mathbb{O}(L^2M)$ is much smaller than $\mathbb{O}(L^2D)$.

\end{enumerate}

\vspace{12pt}

\center\large{\textbf{Question 2}}
\center\textbf{Image classification with CNNs}

\begin{enumerate}[leftmargin=\labelsep]

    \item \textit{Implement a simple convolutional network. Train your model for 15 epochs using SGD tuning only the learning rate on your validation data, using the following values: $0.1$, $0.01$, $0.001$. Report the learning rate of best configuration and plot two things: the training loss and the validation accuracy, both as a function of the epoch number.}

          \vspace{12pt}

          The best configuration, in terms of test accuracy, has a learning rate of $\eta = 0.01$, as shown in Table \ref{tab:cnn-pool-test-acc}.

          \begin{table}[H]
              \centering
              \begin{tabular}{|c|c|}
                  \hline
                  \textbf{Learning Rate} & \textbf{Test Accuracy} \\ \hline
                  0.1                    & 0.7864                 \\ \hline
                  0.01                   & 0.8318                 \\ \hline
                  0.001                  & 0.7864                 \\ \hline
              \end{tabular}
              \caption{Final CNN test accuracies for each learning rate.}
              \label{tab:cnn-pool-test-acc}
          \end{table}

          The training loss and validation accuracy for the best configuration are plotted in Figures \ref{fig:cnn-training-loss-0.01-0.7-0-sgd-false} and \ref{fig:cnn-validation-accuracy-0.01-0.7-0-sgd-false}, respectively.

          \begin{figure}[H]
              \centering
              \includesvg[width=0.75\textwidth]{./2/report/assets/CNN-training-loss-0.01-0.7-0-sgd-False.svg}
              \caption{Training loss as a function of the epoch number for $\eta = 0.01$.}
              \label{fig:cnn-training-loss-0.01-0.7-0-sgd-false}
          \end{figure}

          \begin{figure}[H]
              \centering
              \includesvg[width=0.75\textwidth]{./2/report/assets/CNN-validation-accuracy-0.01-0.7-0-sgd-False.svg}
              \caption{Validation accuracy as a function of the epoch number for $\eta = 0.01$.}
              \label{fig:cnn-validation-accuracy-0.01-0.7-0-sgd-false}
          \end{figure}

    \item \textit{Implement and asses a similar network but where the max-pooling layers were removed and \texttt{self.conv1} and \texttt{self.conv2} are different. \texttt{self.conv1} is a convolution layer with 8 output channels, a kernel of size 3x3, padding of 1 and stride of 2, and \texttt{self.conv2} is a convolution layer with 16 output channels, a kernel of size 3x3, padding of 0, and stride of 2. Modify the \texttt{\_\_init\_\_} and the forward functions to use the \texttt{no\_maxpool} variable and ensure the ability to switch between current and previous definitions of the \texttt{self.conv1} and \texttt{self.conv2} layers and application or not of the max-pooling layer. Report the performance of this network using the optimal hyper-parameters defined in the previous question.}

          \vspace{12pt}

          The best configuration, with the pooling layers removed, in terms of test accuracy, has a learning rate of $\eta = 0.01$, as shown in Table \ref{tab:cnn-test-acc}.

          \begin{table}[H]
              \centering
              \begin{tabular}{|c|c|}
                  \hline
                  \textbf{Learning Rate} & \textbf{Test Accuracy} \\ \hline
                  0.1                    & 0.7580                 \\ \hline
                  0.01                   & 0.7958                 \\ \hline
                  0.001                  & 0.7127                 \\ \hline
              \end{tabular}
              \caption{Final CNN (with no pooling layers) test accuracies for each learning rate.}
              \label{tab:cnn-test-acc}
          \end{table}

          The training loss and validation accuracy for the best configuration without pooling layers are plotted in Figures \ref{fig:cnn-training-loss-0.01-0.7-0-sgd-true} and \ref{fig:cnn-validation-accuracy-0.01-0.7-0-sgd-true}, respectively.

          \begin{figure}[H]
              \centering
              \includesvg[width=0.75\textwidth]{./2/report/assets/CNN-training-loss-0.01-0.7-0-sgd-True.svg}
              \caption{Training loss as a function of the epoch number for $\eta = 0.01$.}
              \label{fig:cnn-training-loss-0.01-0.7-0-sgd-true}
          \end{figure}

          \begin{figure}[H]
              \centering
              \includesvg[width=0.75\textwidth]{./2/report/assets/CNN-validation-accuracy-0.01-0.7-0-sgd-True.svg}
              \caption{Validation accuracy as a function of the epoch number for $\eta = 0.01$.}
              \label{fig:cnn-validation-accuracy-0.01-0.7-0-sgd-true}
          \end{figure}

    \item \textit{Implement the function \texttt{get\_number\_trainable\_params} to determine the number of trainable parameters of CNNs from the two previous questions. What justifies the difference in terms of performance between the networks?}

          \vspace{12pt}

          The number of trainable parameters for both the CNNs with the max pooling layres and the CNNs without the max pooling layers is $224892$.

          \vspace{12pt}

          The difference in performance between the two network may be justified by the fact that max pooling layers aggregate the information from the previous convolutional layer in a way that is invariant to small translations, rotation and scales of the input. This means that the network with the max pooling layers is more robust to small changes to the input, which is useful for image classification tasks, like the one at hand, conferring the models with pooling layers a better performance.

\end{enumerate}

\center\large{\textbf{Question 3}}
\center\textbf{Automatic Speech Recognition}

\begin{enumerate}[leftmargin=\labelsep]

    \item \textit{For the LSTM, plot the comparison of the training and validation loss over epochs and the string similarity scores obtained in the validation set. Also, report the final test loss and the string similarity scores obtained with the test set.}

          \vspace{12pt}

          The comparison of the training and validation losses over epochs are plotted in Figure \ref{fig:attention-rnn-loss-comparison} while the Jaccard, cosine and Damerau-Levenshtein string similarity scores obtained in the validation set are represented, respectively, in Figures \ref{fig:attention-rnn-jaccard}, \ref{fig:attention-rnn-cosine} and \ref{fig:attention-rnn-damerau-levenshtein}.

          \begin{figure}[H]
              \centering
              \includesvg[width=0.75\textwidth]{./2/report/assets/attention-rnn_loss__train_val.svg}
              \caption{Training and validation losses over epochs of the LSTM.}
              \label{fig:attention-rnn-loss-comparison}
          \end{figure}

          \begin{figure}[H]
              \centering
              \includesvg[width=0.75\textwidth]{./2/report/assets/attention-rnn_jaccard_similarity__val.svg}
              \caption{Jaccard similarity scores in the validation set over epochs of the LSTM.}
              \label{fig:attention-rnn-jaccard}
          \end{figure}

          \begin{figure}[H]
              \centering
              \includesvg[width=0.75\textwidth]{./2/report/assets/attention-rnn_cosine_similarity__val.svg}
              \caption{Cosine similarity scores in the validation set over epochs of the LSTM.}
              \label{fig:attention-rnn-cosine}
          \end{figure}

          \begin{figure}[H]
              \centering
              \includesvg[width=0.75\textwidth]{./2/report/assets/attention-rnn_damerau-levenshtein_similarity__val.svg}
              \caption{Damerau-Levenshtein similarity scores in the validation set over epochs of the LSTM.}
              \label{fig:attention-rnn-damerau-levenshtein}
          \end{figure}

          The final test loss is $1.1828277518109578$. The string similarity scores obtained with the test set are depicted in Table \ref{tab:attention-rnn-test-scores}.

          \vspace{12pt}

          \begin{table}[H]
              \centering
              \begin{tabular}{|c|c|}
                  \hline
                  \textbf{String Similarity Metric} & \textbf{Score}     \\ \hline
                  Jaccard                           & 0.714888785108462  \\ \hline
                  Cosine                            & 0.8323868545583469 \\ \hline
                  Damerau-Levenshtein               & 0.5086983165244969 \\ \hline
              \end{tabular}
              \caption{String similarity score of the LSTM.}
              \label{tab:attention-rnn-test-scores}
          \end{table}

    \item \textit{For the attention mechanism, plot the comparison of the training and validation loss over epochs and the string similarity scores obtained in the validation set. Also, report the final test loss and the string similarity scores obtained with the test set.}

          \vspace{12pt}

          The comparison of the training and validation losses over epochs are plotted in Figure \ref{fig:attention-attention-loss-comparison} while the Jaccard, cosine and Damerau-Levenshtein string similarity scores obtained in the validation set are represented, respectively, in Figures \ref{fig:attention-attention-jaccard}, \ref{fig:attention-attention-cosine} and \ref{fig:attention-attention-damerau-levenshtein}.

          \begin{figure}[H]
              \centering
              \includesvg[width=0.75\textwidth]{./2/report/assets/attention-attention_loss__train_val.svg}
              \caption{Training and validation losses over epochs of the attention mechanism.}
              \label{fig:attention-attention-loss-comparison}
          \end{figure}

          \begin{figure}[H]
              \centering
              \includesvg[width=0.75\textwidth]{./2/report/assets/attention-attention_jaccard_similarity__val.svg}
              \caption{Jaccard similarity scores in the validation set over epochs of the attention mechanism.}
              \label{fig:attention-attention-jaccard}
          \end{figure}

          \begin{figure}[H]
              \centering
              \includesvg[width=0.75\textwidth]{./2/report/assets/attention-attention_cosine_similarity__val.svg}
              \caption{Cosine similarity scores in the validation set over epochs of the attention mechanism.}
              \label{fig:attention-attention-cosine}
          \end{figure}

          \begin{figure}[H]
              \centering
              \includesvg[width=0.75\textwidth]{./2/report/assets/attention-attention_damerau-levenshtein_similarity__val.svg}
              \caption{Damerau-Levenshtein similarity scores in the validation set over epochs of the attention mechanism.}
              \label{fig:attention-attention-damerau-levenshtein}
          \end{figure}

          \vspace{12pt}

          The final test loss is $1.1603884776917899$. The string similarity scores obtained with the test set are depicted in Table \ref{tab:attention-attention-test-scores}.

          \begin{table}[H]
              \centering
              \begin{tabular}{|c|c|}
                  \hline
                  \textbf{String Similarity Metric} & \textbf{Score}     \\ \hline
                  Jaccard                           & 0.7644286566881149  \\ \hline
                  Cosine                            & 0.865211054644348 \\ \hline
                  Damerau-Levenshtein               & 0.6329973770804788 \\ \hline
              \end{tabular}
              \caption{String similarity score of the attention mechanism.}
              \label{tab:attention-attention-test-scores}
          \end{table}

    \item \textit{Comment on the differences in the test results obtained using the decoder architectures in questions 1 and 2. Note: start by illustrating how the LSTM (in question 1) and the attention mechanism (in question 2) process the text input.}

          \vspace{12pt}

          LSTMs have a memory cell that allows them to capture long-term dependencies in the input sequence. They use gates to control the flow of information into and out of the cell, making them capable of learning and retaining information over extended periods.

          The attention mechanism allows the model to selectively focus on different parts of the input sequence when generating each element of the output sequence. Attention helps the model selectively attend to relevant information, providing a dynamic way to weigh the importance of different parts of the input sequence at each step of decoding.

          The attention mechanism outperforms the LSTM as it is better processing long sequences and capturing textual dependencies. On the other hand, the LSTM needs to \textit{remember} past information, which is not as efficient as the attention mechanism.

    \item \textit{Comment on the score values obtained by each string similarity score used. Why each score has different values based on what each similarity tries to evaluate?}

          \vspace{12pt}

          The different score values arise from the distinct characteristics and underlying principles of each similarity metric. The more appropriate metric depends on the use case.

          \vspace{12pt}

          Jaccard similarity compares the intersection of two sets to their union. The resulting value, ranging from 0 to 1, represents the proportion of shared elements between the sets.

          \vspace{12pt}

          Cosine similarity measures the cosine of the angle between two vectors that represent the two strings.

          \vspace{12pt}

          Damerau-Levenshtein similarity quantifies the similarity between two strings based on the minimum number of edit operations (insertions, deletions, substitutions, and transpositions) needed to transform one string into the other.

\end{enumerate}

\end{document}
